{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Your First Machine Learning Project - Part 1 (Google Colab Edition)\n",
    "# Data Wrangling with Gemini API\n",
    "\n",
    "This notebook demonstrates how to build a complete machine learning project using Google Colab and Gemini API.\n",
    "\n",
    "## What We'll Cover:\n",
    "\n",
    "1. **Data Loading** - Load the bear dataset from GitHub using pandas\n",
    "2. **Image Analysis with Gemini** - Extract features from bear images using Gemini's vision capabilities\n",
    "3. **Feature Extraction** - Analyze fur color, facial profile, and paw pad texture\n",
    "4. **Data Preparation** - Combine all features into a final dataset\n",
    "5. **Data Export** - Save the processed data to CSV files\n",
    "\n",
    "## Prerequisites:\n",
    "- Google Colab account\n",
    "- Gemini API key (free from [Google AI Studio](https://makersuite.google.com/app/apikey))\n",
    "- Store your API key in Colab Secrets as `BEAR_ML_KEY`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, we'll install the necessary packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q google-generativeai pandas pillow requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure API Key from Colab Secrets\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Before running this cell:\n",
    "1. Click the üîë key icon in the left sidebar\n",
    "2. Click \"Add new secret\"\n",
    "3. Name: `BEAR_ML_KEY`\n",
    "4. Value: Your Gemini API key\n",
    "5. Toggle \"Notebook access\" ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "\n",
    "# Get API key from Colab secrets\n",
    "try:\n",
    "    api_key = userdata.get('BEAR_ML_KEY')\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"‚úÖ API key loaded successfully from Colab secrets!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error loading API key. Make sure you've added BEAR_ML_KEY to Colab secrets.\")\n",
    "    print(f\"Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Bear Dataset\n",
    "\n",
    "Load the raw bear data from GitHub containing physical measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bear dataset from GitHub\n",
    "base_url = \"https://raw.githubusercontent.com/dataprofessor/bear-dataset/master/\"\n",
    "df = pd.read_csv(base_url + \"bear_raw_data.csv\")\n",
    "\n",
    "print(\"üìä Bear Dataset Loaded:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Rate Limiter\n",
    "\n",
    "We'll create a rate limiter that ensures we don't exceed 10 requests per minute (free tier limit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimiter:\n",
    "    \"\"\"\n",
    "    Rate limiter to ensure we don't exceed Gemini free tier limits.\n",
    "    Free tier: 10 requests per minute (RPM)\n",
    "    \"\"\"\n",
    "    def __init__(self, max_requests_per_minute=10):\n",
    "        self.max_rpm = max_requests_per_minute\n",
    "        self.min_interval = 60.0 / max_requests_per_minute  # seconds between requests\n",
    "        self.last_request_time = 0\n",
    "        self.request_count = 0\n",
    "        \n",
    "    def wait_if_needed(self):\n",
    "        \"\"\"Wait if necessary to respect rate limits\"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last = current_time - self.last_request_time\n",
    "        \n",
    "        if time_since_last < self.min_interval:\n",
    "            wait_time = self.min_interval - time_since_last\n",
    "            print(f\"‚è≥ Rate limiting: waiting {wait_time:.1f}s...\")\n",
    "            time.sleep(wait_time)\n",
    "        \n",
    "        self.last_request_time = time.time()\n",
    "        self.request_count += 1\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get statistics about API usage\"\"\"\n",
    "        return {\n",
    "            'total_requests': self.request_count,\n",
    "            'max_rpm': self.max_rpm\n",
    "        }\n",
    "\n",
    "# Initialize rate limiter\n",
    "rate_limiter = RateLimiter(max_requests_per_minute=10)\n",
    "print(\"‚úÖ Rate limiter initialized (10 requests/minute max)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Gemini Model and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemini model (using Flash for faster, free tier)\n",
    "model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "def load_image_from_url(url):\n",
    "    \"\"\"Load an image from a URL\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return Image.open(BytesIO(response.content))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading image from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_bear_image(image_url, prompt, bear_id):\n",
    "    \"\"\"\n",
    "    Analyze a bear image using Gemini API with rate limiting.\n",
    "    \n",
    "    Args:\n",
    "        image_url: URL of the bear image\n",
    "        prompt: Analysis prompt for Gemini\n",
    "        bear_id: ID of the bear being analyzed\n",
    "    \n",
    "    Returns:\n",
    "        Analysis result as string\n",
    "    \"\"\"\n",
    "    # Apply rate limiting\n",
    "    rate_limiter.wait_if_needed()\n",
    "    \n",
    "    # Load image\n",
    "    image = load_image_from_url(image_url)\n",
    "    if image is None:\n",
    "        return \"Error: Could not load image\"\n",
    "    \n",
    "    try:\n",
    "        # Generate content with Gemini\n",
    "        response = model.generate_content([prompt, image])\n",
    "        result = response.text.strip()\n",
    "        print(f\"‚úì Analyzed {bear_id}: {result}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing {bear_id}: {e}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Gemini model and helper functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Display Sample Bear Images\n",
    "\n",
    "Let's look at examples of each bear species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPImage, display, HTML\n",
    "\n",
    "# Bear species information\n",
    "bears = [\n",
    "    (\"ABB\", \"American Black Bear\"),\n",
    "    (\"EUR\", \"Eurasian Brown Bear\"), \n",
    "    (\"GRZ\", \"Grizzly Bear\"),\n",
    "    (\"KDK\", \"Kodiak Bear\")\n",
    "]\n",
    "\n",
    "image_base_url = \"https://github.com/dataprofessor/bear-dataset/blob/master/images/\"\n",
    "\n",
    "print(\"üêª Sample Bear Species:\\n\")\n",
    "\n",
    "html_content = '<div style=\"display: flex; justify-content: space-around;\">'\n",
    "for species, name in bears:\n",
    "    image_url = f\"{image_base_url}{species}_01.png?raw=true\"\n",
    "    html_content += f'''\n",
    "    <div style=\"text-align: center; margin: 10px;\">\n",
    "        <img src=\"{image_url}\" width=\"200\"/>\n",
    "        <p><strong>{name}</strong><br/>({species}_01.png)</p>\n",
    "    </div>\n",
    "    '''\n",
    "html_content += '</div>'\n",
    "\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Extraction: Fur Color\n",
    "\n",
    "Now we'll use Gemini to analyze the fur color of each bear.\n",
    "\n",
    "‚ö†Ô∏è **Note**: This will take approximately 20 minutes to process all 200 images (10 requests/minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fur color analysis prompt\n",
    "fur_color_prompt = \"\"\"\n",
    "Analyze the provided image of a bear. Describe only the fur color of the bear\n",
    "by choosing the most appropriate term from the following list. The response\n",
    "should be a single value with no explanation.\n",
    "\n",
    "Choose from:\n",
    "- Light Brown\n",
    "- Medium Brown\n",
    "- Blond\n",
    "- Dark Brown\n",
    "- Grizzled\n",
    "- Reddish Brown\n",
    "- Blackish Brown\n",
    "- Black\n",
    "- Brown\n",
    "- Cinnamon\n",
    "\n",
    "Respond with ONLY the color name.\n",
    "\"\"\"\n",
    "\n",
    "# Test with first image\n",
    "print(\"üß™ Testing with first bear image...\\n\")\n",
    "test_id = df['id'].iloc[0]\n",
    "test_url = f\"https://raw.githubusercontent.com/dataprofessor/bear-dataset/master/images/{test_id}.png\"\n",
    "test_result = analyze_bear_image(test_url, fur_color_prompt, test_id)\n",
    "print(f\"\\n‚úÖ Test successful! Result: {test_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all images for fur color\n",
    "# WARNING: This will take ~20 minutes for 200 images\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üé® ANALYZING FUR COLOR FOR ALL BEARS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total images to process: {len(df)}\")\n",
    "print(f\"Estimated time: ~{len(df) * 6 / 60:.0f} minutes\")\n",
    "print(f\"Rate limit: 10 requests/minute\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "fur_color_results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    bear_id = row['id']\n",
    "    image_url = f\"https://raw.githubusercontent.com/dataprofessor/bear-dataset/master/images/{bear_id}.png\"\n",
    "    \n",
    "    # Analyze fur color\n",
    "    fur_color = analyze_bear_image(image_url, fur_color_prompt, bear_id)\n",
    "    \n",
    "    # Store result\n",
    "    fur_color_results.append({\n",
    "        'id': bear_id,\n",
    "        'fur_color': fur_color\n",
    "    })\n",
    "    \n",
    "    # Progress update every 10 images\n",
    "    if (idx + 1) % 10 == 0:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds() / 60\n",
    "        remaining = ((len(df) - idx - 1) * 6) / 60\n",
    "        print(f\"\\nüìä Progress: {idx + 1}/{len(df)} ({(idx + 1)/len(df)*100:.1f}%)\")\n",
    "        print(f\"   Elapsed: {elapsed:.1f}m | Remaining: {remaining:.1f}m\\n\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_fur_color = pd.DataFrame(fur_color_results)\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_time = (end_time - start_time).total_seconds() / 60\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ FUR COLOR ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total time: {total_time:.1f} minutes\")\n",
    "print(f\"Total API calls: {rate_limiter.get_stats()['total_requests']}\")\n",
    "print(f\"\\nResults preview:\")\n",
    "display(df_fur_color.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Extraction: Facial Profile\n",
    "\n",
    "Next, we'll analyze the facial profile of each bear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facial profile analysis prompt\n",
    "facial_profile_prompt = \"\"\"\n",
    "Analyze the provided image of a bear. Describe only the facial profile of the bear.\n",
    "The response must be one of the following two values as a single word with no explanation:\n",
    "\n",
    "- Dished (Concave profile, where the bridge of the nose dips)\n",
    "- Straight (Flat profile, with no dip from the forehead to the nose)\n",
    "\n",
    "Respond with ONLY one word: either \"Dished\" or \"Straight\".\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üëÉ ANALYZING FACIAL PROFILE FOR ALL BEARS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total images to process: {len(df)}\")\n",
    "print(f\"Estimated time: ~{len(df) * 6 / 60:.0f} minutes\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "facial_profile_results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    bear_id = row['id']\n",
    "    image_url = f\"https://raw.githubusercontent.com/dataprofessor/bear-dataset/master/images/{bear_id}.png\"\n",
    "    \n",
    "    # Analyze facial profile\n",
    "    facial_profile = analyze_bear_image(image_url, facial_profile_prompt, bear_id)\n",
    "    \n",
    "    # Store result\n",
    "    facial_profile_results.append({\n",
    "        'id': bear_id,\n",
    "        'facial_profile': facial_profile\n",
    "    })\n",
    "    \n",
    "    # Progress update every 10 images\n",
    "    if (idx + 1) % 10 == 0:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds() / 60\n",
    "        remaining = ((len(df) - idx - 1) * 6) / 60\n",
    "        print(f\"\\nüìä Progress: {idx + 1}/{len(df)} ({(idx + 1)/len(df)*100:.1f}%)\")\n",
    "        print(f\"   Elapsed: {elapsed:.1f}m | Remaining: {remaining:.1f}m\\n\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_facial_profile = pd.DataFrame(facial_profile_results)\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_time = (end_time - start_time).total_seconds() / 60\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ FACIAL PROFILE ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total time: {total_time:.1f} minutes\")\n",
    "print(f\"\\nResults preview:\")\n",
    "display(df_facial_profile.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Extraction: Paw Pad Texture\n",
    "\n",
    "Finally, we'll analyze the paw pad texture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paw pad texture analysis prompt\n",
    "paw_pad_prompt = \"\"\"\n",
    "Analyze the provided image of a bear. Describe only the paw pad texture of the bear.\n",
    "The response must be one of the following two values as a single word with no explanation:\n",
    "\n",
    "- Smooth (Less textured and relatively flat, for walking)\n",
    "- Rough (More textured and grooved, for gripping and climbing)\n",
    "\n",
    "Respond with ONLY one word: either \"Smooth\" or \"Rough\".\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üêæ ANALYZING PAW PAD TEXTURE FOR ALL BEARS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total images to process: {len(df)}\")\n",
    "print(f\"Estimated time: ~{len(df) * 6 / 60:.0f} minutes\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "paw_pad_results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    bear_id = row['id']\n",
    "    image_url = f\"https://raw.githubusercontent.com/dataprofessor/bear-dataset/master/images/{bear_id}.png\"\n",
    "    \n",
    "    # Analyze paw pad texture\n",
    "    paw_pad = analyze_bear_image(image_url, paw_pad_prompt, bear_id)\n",
    "    \n",
    "    # Store result\n",
    "    paw_pad_results.append({\n",
    "        'id': bear_id,\n",
    "        'paw_pad_texture': paw_pad\n",
    "    })\n",
    "    \n",
    "    # Progress update every 10 images\n",
    "    if (idx + 1) % 10 == 0:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds() / 60\n",
    "        remaining = ((len(df) - idx - 1) * 6) / 60\n",
    "        print(f\"\\nüìä Progress: {idx + 1}/{len(df)} ({(idx + 1)/len(df)*100:.1f}%)\")\n",
    "        print(f\"   Elapsed: {elapsed:.1f}m | Remaining: {remaining:.1f}m\\n\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_paw_pad = pd.DataFrame(paw_pad_results)\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_time = (end_time - start_time).total_seconds() / 60\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ PAW PAD TEXTURE ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total time: {total_time:.1f} minutes\")\n",
    "print(f\"\\nResults preview:\")\n",
    "display(df_paw_pad.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Combine All Features\n",
    "\n",
    "Now we'll merge all the extracted features with the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize IDs (ensure uppercase)\n",
    "df['id'] = df['id'].str.upper()\n",
    "df_fur_color['id'] = df_fur_color['id'].str.upper()\n",
    "df_facial_profile['id'] = df_facial_profile['id'].str.upper()\n",
    "df_paw_pad['id'] = df_paw_pad['id'].str.upper()\n",
    "\n",
    "# Merge all features\n",
    "df_combined = df.merge(df_fur_color, on='id', how='inner')\n",
    "df_combined = df_combined.merge(df_facial_profile, on='id', how='inner')\n",
    "df_combined = df_combined.merge(df_paw_pad, on='id', how='inner')\n",
    "\n",
    "print(\"‚úÖ All features combined successfully!\")\n",
    "print(f\"\\nFinal dataset shape: {df_combined.shape}\")\n",
    "print(f\"Columns: {df_combined.columns.tolist()}\")\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "display(df_combined.head(10))\n",
    "\n",
    "# Check for any missing values\n",
    "print(\"\\nüìä Missing values check:\")\n",
    "print(df_combined.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary Statistics\n",
    "\n",
    "Let's examine the distribution of extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Feature Distribution Summary\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fur color distribution\n",
    "print(\"\\nüé® Fur Color Distribution:\")\n",
    "print(df_combined['fur_color'].value_counts())\n",
    "\n",
    "# Facial profile distribution\n",
    "print(\"\\nüëÉ Facial Profile Distribution:\")\n",
    "print(df_combined['facial_profile'].value_counts())\n",
    "\n",
    "# Paw pad texture distribution\n",
    "print(\"\\nüêæ Paw Pad Texture Distribution:\")\n",
    "print(df_combined['paw_pad_texture'].value_counts())\n",
    "\n",
    "# Species distribution\n",
    "print(\"\\nüêª Species Distribution:\")\n",
    "print(df_combined['species'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Export Data\n",
    "\n",
    "Save the processed data to CSV files for use in subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined dataset\n",
    "df_combined.to_csv('bear_data_complete.csv', index=False)\n",
    "print(\"‚úÖ Saved complete dataset to: bear_data_complete.csv\")\n",
    "\n",
    "# Save individual feature files\n",
    "df_fur_color.to_csv('fur_color_extracted.csv', index=False)\n",
    "df_facial_profile.to_csv('facial_profile_extracted.csv', index=False)\n",
    "df_paw_pad.to_csv('paw_pad_texture_extracted.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Saved individual feature files:\")\n",
    "print(\"   - fur_color_extracted.csv\")\n",
    "print(\"   - facial_profile_extracted.csv\")\n",
    "print(\"   - paw_pad_texture_extracted.csv\")\n",
    "\n",
    "# Download files to local machine\n",
    "print(\"\\nüíæ To download files to your computer:\")\n",
    "print(\"   1. Click the folder icon in the left sidebar\")\n",
    "print(\"   2. Right-click on the CSV files\")\n",
    "print(\"   3. Select 'Download'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. API Usage Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = rate_limiter.get_stats()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä GEMINI API USAGE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal API Requests: {stats['total_requests']}\")\n",
    "print(f\"Rate Limit: {stats['max_rpm']} requests/minute\")\n",
    "print(f\"\\nFeatures Extracted:\")\n",
    "print(f\"  ‚Ä¢ Fur Color: ‚úÖ\")\n",
    "print(f\"  ‚Ä¢ Facial Profile: ‚úÖ\")\n",
    "print(f\"  ‚Ä¢ Paw Pad Texture: ‚úÖ\")\n",
    "print(f\"\\nTotal Bears Analyzed: {len(df_combined)}\")\n",
    "print(\"\\n‚úÖ All analysis complete! Ready for machine learning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Next Steps\n",
    "\n",
    "Now that you have the complete dataset with extracted features, you can:\n",
    "\n",
    "1. **Exploratory Data Analysis (EDA)**: Visualize distributions and correlations\n",
    "2. **Feature Engineering**: Create additional features or transformations\n",
    "3. **Machine Learning**: Train classification models (Random Forest, SVM, Logistic Regression)\n",
    "4. **Model Evaluation**: Compare model performance\n",
    "5. **Deployment**: Create a Streamlit app for predictions\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Gemini API Documentation](https://ai.google.dev/docs)\n",
    "- [Bear Dataset GitHub](https://github.com/dataprofessor/bear-dataset)\n",
    "- [scikit-learn Documentation](https://scikit-learn.org/)\n",
    "- [Pandas Documentation](https://pandas.pydata.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
